{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["zeJi-xfkGXxa","d32HFAdyHg5X","mnvzBnqqJZM3","Q4hXJgcTarIH","9En-6Gp_Kuc4","_G-leKJPl1Yi","_av2hprdhRnO","ubfnWSC0GK5F","uyyrr_SrSNWH","pr0fRWVrxlXx","pm7I3TU8TYkN","vv5pfk7daayd","npFVBOVCaLnB","bxFeaTdQhc_n","Ss9bdYigRp4t","6plJXJGyrvJn","Sdeqxkv0nlZe","UVhSXWHynlZo","pHV3Zel-nlZs"],"mount_file_id":"1jE4TncvILEmuCZ_0STKvNbS85XPfO0oL","authorship_tag":"ABX9TyP/OWLGX9t/D2yP0HWVaJeN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Libraries"],"metadata":{"id":"zeJi-xfkGXxa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RDHmh7sGPyE"},"outputs":[],"source":["!pip uninstall pydicom -y\n","!pip install pydicom\n","\n","!pip install pillow>=10.0\n","!pip install pylibjpeg>=2.0\n","!pip install pylibjpeg-libjpeg>=2.1\n","!pip install gdcm>=3.0.10"]},{"cell_type":"code","source":["!pip install aif360\n","!pip install aif360[inFairness]\n","!pip install aif360[OptimalTransport]"],"metadata":{"id":"WH1dadTpgvaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fairlearn"],"metadata":{"id":"yyli69TGnlZt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import os\n","import pydicom\n","import json\n","import numpy as np\n","from skimage.transform import resize\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict\n","import itertools\n","import time\n","\n","# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import os\n","import pydicom\n","import json\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow import keras\n","\n","from skimage.transform import resize\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import cv2 as cv\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"metadata":{"id":"nfoJkExLHb8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"d32HFAdyHg5X"}},{"cell_type":"markdown","source":["This section contains the functios required for reading the dicom image files, annotations, and creating GradCAM visualization heatmaps."],"metadata":{"id":"xe556_ekcuwB"}},{"cell_type":"code","source":["error_summary = defaultdict(int)\n","error_details = defaultdict(list)\n","\n","# Loading annotations\n","def load_annotations(json_file):\n","    try:\n","        with open(json_file) as f:\n","            annotations = json.load(f)\n","        return annotations\n","    except Exception as e:\n","        error_summary['Annotation Load Errors'] += 1\n","        error_details['Annotation Load Errors'].append((json_file, str(e)))\n","        return []\n","\n","# Loading DICOM images\n","## The images are resized and converted to\n","## 3-channel images by stacking them\n","def load_dicom_image(file_path, img_size):\n","    try:\n","        dicom = pydicom.dcmread(file_path)\n","        img = dicom.pixel_array\n","        img = resize(img, (img_size, img_size), mode='constant', anti_aliasing=True)\n","        img = np.stack((img,)*3, axis=-1)  # Convert to 3-channel image\n","        return img\n","    except Exception as e:\n","        print(\"DICOM Load Errors\")\n","        error_summary['DICOM Load Errors'] += 1\n","        error_details['DICOM Load Errors'].append((file_path, str(e)))\n","        return None\n","\n","# Recursively loading all DICOM files from the given directories\n","def load_dicom_files_from_folder(folder, max_files=10):\n","    try:\n","        files = glob.glob(os.path.join(folder, '**', '*.dcm'), recursive=True)\n","        return files[:max_files]  # Limit to a subset for testing\n","    except Exception as e:\n","        print(\"DICOM Folder Load Errors\")\n","        error_summary['DICOM Folder Load Errors'] += 1\n","        error_details['DICOM Folder Load Errors'].append((folder, str(e)))\n","        return []"],"metadata":{"id":"b7hgphxFHjFS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## The GradCAM function for creating the heatmap visualizations for explainability purposes and evaluations\n","## The function is according to the Keras documents\n","def gradcam(img_array, model, last_conv_layer_name, model_last_layer, pred_index=None):\n","    grad_model = keras.models.Model(\n","        model.inputs, [model.get_layer(last_conv_layer_name).output,\n","                       model.get_layer(model_last_layer).output])\n","\n","    # Top predicted class; the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # gradient of the output neuron; feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # mean intensity of the gradient over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","    # this is to handle zero gradients\n","    epsilon = 1e-8\n","    pooled_grads = tf.maximum(pooled_grads, epsilon)\n","\n","    # multiply each channel in the feature map array by its importance\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    # normalize heatmap\n","    heatmap = tf.maximum(heatmap, 0)\n","    heatmap /= (tf.math.reduce_max(heatmap) + epsilon)\n","    return heatmap.numpy()"],"metadata":{"id":"tgDLNOdW4vKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reading Images"],"metadata":{"id":"mnvzBnqqJZM3"}},{"cell_type":"markdown","source":["This section loads the images from the dataset directories. The data is sampled for the first 1000 instances in sample directories.\n","\n","Loading and Creating the datasets can take a while. Thus, please be patient."],"metadata":{"id":"akLo3wVPd96Q"}},{"cell_type":"code","source":["sample_size = 200\n","img_size = 128"],"metadata":{"id":"H5CKUcngwxc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Paths to directories containing DICOM files\n","image_dirs = [\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/AL Codes and Data/Dataset/mdai_public_project_LxR6zdR2_images_2018-08-20-184248\",\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/AL Codes and Data/Dataset/mdai_rsna_project_x9N20BZa_images_2018-07-20-153330\"\n","]\n","\n","image_dirs_samples = [\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/AL Codes and Data/Dataset/samples_public\",\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/AL Codes and Data/Dataset/samples_rsna\"\n","]\n","\n","# Load a subset of all DICOM files from the specified directories (sampled instances)\n","directory = image_dirs_samples"],"metadata":{"id":"r5ycFOBKJbDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Counting the number of files and folders in the pneumonia dataset directories\n","# import os\n","# for dir in image_dirs_samples:\n","#   print(dir)\n","#   files = folders = 0\n","#   for _, dirnames, filenames in os.walk(dir):\n","#       files += len(filenames)\n","#       folders += len(dirnames)\n","#   print(\"{:,} files, {:,} folders\".format(files, folders))\n","\n","\n","# # ## (1000, 1010) files (for now, as samples)"],"metadata":{"id":"qPJEpJDGGyEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Loading dicom files from the specified directories\n","## each directory is loaded individually in a dictionary format\n","dicom_files = dict()\n","for img_dir in directory:\n","  dicom_files[directory.index(img_dir)] = load_dicom_files_from_folder(img_dir, max_files=sample_size)\n","  print(error_summary, error_details)\n","  print(f\"Dataset {directory.index(img_dir)} has completed.\")"],"metadata":{"id":"PAndm4QzK4h-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dicom_files[0]), len(dicom_files[1])"],"metadata":{"id":"Z3kcjo5Fm4e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## loading the images and extracting the relevant information from the dicom files\n","## each directory is loaded seperately to the Dataset dictionary.\n","## the information extracted from the dicom files include patient's gender, age,\n","## and the study_instance_id which is used to map the images to their annotations\n","Dataset = dict()\n","for key in dicom_files.keys():\n","  images = []\n","  for file_ in dicom_files[key]:\n","    dcm = pydicom.dcmread(file_)\n","    img = load_dicom_image(file_, img_size)\n","    gender = dcm.PatientSex\n","    age = dcm.PatientAge\n","    studyinstaneid = dcm.StudyInstanceUID\n","    images.append([studyinstaneid, img, gender, int(age)])\n","  Dataset[key] = images\n","  print(f\"Dataset {key} has completed.\")"],"metadata":{"id":"cfWhyUHjd_9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## To prevent any changes on the original loaded data, we copy the dataset dictionary as a backup file.\n","## Further in the code, in case we require the original data to be restored,\n","## we won't have to load the dicom files, which is time-consuming\n","import copy\n","Dataset_backup = copy.deepcopy(Dataset)"],"metadata":{"id":"dHU95GNTzFnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plotting a Sample Image"],"metadata":{"id":"Q4hXJgcTarIH"}},{"cell_type":"code","source":["# Displaying the first 9 images as for sample illustrations\n","plt.figure(figsize=(20,10))\n","\n","for i in range(9):\n","    plt.subplot(3, 3, i + 1)\n","\n","    image_sample = pydicom.dcmread(dicom_files[0][i]).pixel_array\n","    plt.imshow(image_sample, cmap='gray')\n","    plt.axis('off')\n","\n","plt.tight_layout()"],"metadata":{"id":"L3G-i3goqm47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reading Annotations"],"metadata":{"id":"9En-6Gp_Kuc4"}},{"cell_type":"markdown","source":["In this section, we load annotations and map them to the images according to their study instance IDs. These IDs are unique, so that the mapping can be done without problem."],"metadata":{"id":"yBRL-sjUvCZW"}},{"cell_type":"code","source":["# Paths to annotation files\n","annotation_files = [\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/Projects-Ms.Movahed/Paper-Augmented/main data-original/pneumonia-challenge-annotations-adjudicated-kaggle_2018.json\",\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/Projects-Ms.Movahed/Paper-Augmented/main data-original/pneumonia-challenge-annotations-original_2018.json\",\n","    r\"/content/drive/MyDrive/Folders/+Projects/Dr Shahbazian/Projects-Ms.Movahed/Paper-Augmented/main data-original/pneumonia-challenge-dataset-mappings_2018.json\"\n","]"],"metadata":{"id":"2k-vh0XLKwBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Annotations1 => 'ChestX-ray14 subset: Pneumonia','x9N20BZa' => mdai_rsna_project_x9N20BZa_images_2018-07-20-153330 Dataset\n","## Annotations2 => 'RSNA Pneumonia Detection Challenge on Kaggle', 'LxR6zdR2' => mdai_public_project_LxR6zdR2_images_2018-08-20-184248 Dataset\n","\n","## Annotations 1 => Dataset 1 => rsna\n","## Annotations 2 => Dataset 0 => public\n","\n","annotations1 = load_annotations(annotation_files[0]) ## dict\n","annotations2 = load_annotations(annotation_files[1]) ## dict\n","annotations3 = load_annotations(annotation_files[2]) ## list"],"metadata":{"id":"OOYTb-rtY5QR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# annotations1['labelGroups'][0]['labels']"],"metadata":{"id":"Es24w4HHDALS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# annotations1['datasets'][0]['annotations'][0:10]"],"metadata":{"id":"BR355TEsD-Vv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# annotations2['datasets'][0]['annotations'][0:10]"],"metadata":{"id":"mcYB9W8rFkXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotations1.keys(), annotations1['name'], annotations1['id']"],"metadata":{"id":"4-cD-Jy5hpBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotations2.keys(), annotations2['name'], annotations2['id']"],"metadata":{"id":"-KE9IiDLl31P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotations3[0]"],"metadata":{"id":"cKoE_VAgl5g-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# study_instance_id = [i['StudyInstanceUID'] for i in  annotations3]\n","# series_instance_id = [i['SeriesInstanceUID'] for i in  annotations3]\n","# sop_instance_id = [i['SOPInstanceUID'] for i in  annotations3]\n","\n","# len(study_instance_id) != len(set(study_instance_id)), len(series_instance_id) != len(set(series_instance_id)), len(sop_instance_id) != len(set(sop_instance_id))\n","\n","## All instance ids are unique => can be used as image keys and annotations\n","# (False, False, False)"],"metadata":{"id":"Weii3_HRnFB4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating Dataset"],"metadata":{"id":"_G-leKJPl1Yi"}},{"cell_type":"markdown","source":["In this section, we create the the final version of the datasets by adding the annotations to each data instance.\n","\n","the ratio of images are also used to convert the bounding box to the final resized image."],"metadata":{"id":"PZCwQ84wvTqs"}},{"cell_type":"code","source":["ratio = 1024/img_size\n","print(f\"Image Ratio: {ratio}\")\n","\n","id_annot1 = dict()\n","id_bbox1 = dict()\n","for annotation in annotations1['datasets'][0]['annotations']:\n","    if annotation['annotationNumber'] is None:\n","      label = 0\n","      data_x, data_y, data_width, data_height = 0, 0, 0, 0\n","    else:\n","      label = int(annotation['annotationNumber'])\n","      data_x = annotation['data']['x']/ratio\n","      data_y = annotation['data']['y']/ratio\n","      data_width = annotation['data']['width']/ratio\n","      data_height = annotation['data']['height']/ratio\n","\n","    id_annot1[annotation['StudyInstanceUID']] = label\n","    id_bbox1[annotation['StudyInstanceUID']] = (data_x, data_y, data_width, data_height)\n","\n","\n","id_annot2 = dict()\n","id_bbox2 = dict()\n","for annotation in annotations2['datasets'][0]['annotations']:\n","    if annotation['annotationNumber'] is None:\n","      label = 0\n","      data_x, data_y, data_width, data_height = 0, 0, 0, 0\n","    else:\n","      label = int(annotation['annotationNumber'])\n","      data_x = annotation['data']['x']/ratio\n","      data_y = annotation['data']['y']/ratio\n","      data_width = annotation['data']['width']/ratio\n","      data_height = annotation['data']['height']/ratio\n","\n","    id_annot2[annotation['StudyInstanceUID']] = label\n","    id_bbox2[annotation['StudyInstanceUID']] = (data_x, data_y, data_width, data_height)\n","\n","\n","id_annot3 = dict()\n","for item in annotations3:\n","  id_annot3[item['StudyInstanceUID']] = item['subset_init_label']"],"metadata":{"id":"2rqjz-UJjxBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Dataset = copy.deepcopy(Dataset_backup)\n","## Annotations 1 => Dataset 1\n","## Annotations 2 => Dataset 0\n","for key in Dataset:\n","  for img in Dataset[key]:\n","    # img.append(id_annot3[img[0]])\n","    if key == 0:\n","      img.append(id_annot2[img[0]])\n","      img.append(id_bbox2[img[0]])\n","    elif key == 1:\n","      img.append(id_annot1[img[0]])\n","      img.append(id_bbox1[img[0]])\n","  print(f\"Dataset {key} has completed.\")"],"metadata":{"id":"9_C7byfygIcP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i = 0\n","for item in Dataset[0]:\n","  if item[-2] != 0:\n","    print(f\"The first Pneumonia case is:{i} \\n {item[2:]}\")\n","    break\n","  i += 1"],"metadata":{"id":"GJR0Qh8HHuG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Displaying a sample image with bbox"],"metadata":{"id":"_av2hprdhRnO"}},{"cell_type":"code","source":["# Displaying a sample image with bbox\n","## To change the images you only need to specify the image index and the dataset key,\n","## which refers to samples from either public or rsna directories.\n","## Provided that the data instance is a positive case of pneumonia, there will be a red box on the image\n","fig, ax = plt.subplots()\n","\n","dataset_index = 0\n","img_index = 1\n","print(f\"Annotation: {Dataset[dataset_index][img_index][4]}\")\n","\n","image_sample = Dataset[dataset_index][img_index][1]\n","ax.imshow(image_sample)\n","\n","# Create a Rectangle patch\n","x = Dataset[dataset_index][img_index][-1][0]\n","y = Dataset[dataset_index][img_index][-1][1]\n","width = Dataset[dataset_index][img_index][-1][2]\n","height = Dataset[dataset_index][img_index][-1][3]\n","\n","print(f\"Box: {(x, y, width, height)}\")\n","\n","rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n","\n","ax.add_patch(rect)\n","plt.tight_layout()"],"metadata":{"id":"yyVtAkZ3PT2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Original Model"],"metadata":{"id":"-q_QcpbnKeIE"}},{"cell_type":"markdown","source":["### Data Creation"],"metadata":{"id":"ubfnWSC0GK5F"}},{"cell_type":"code","source":["# Xray_images = [img[1] for img in Dataset[key] for key in Dataset.keys()]\n","# labels = [img[4] for img in Dataset[key] for key in Dataset.keys()]\n","\n","Xray_images = []\n","labels = []\n","genders = []\n","for key in Dataset.keys():\n","  for img in Dataset[key]:\n","    Xray_images.append(img[1])\n","    genders.append({'F':0, 'M':1}[img[2]])\n","    labels.append(img[4])\n","\n","num_classes = len(set(labels))\n","print(\"Num Classes: \", num_classes)\n","\n","Xray_images = np.array(Xray_images)\n","labels = np.array(labels)\n","genders = np.array(genders)\n","\n","Xray_images = np.array(Xray_images)\n","labels = np.array(labels)\n","\n","labels = np.expand_dims(labels,axis=1)\n","genders = np.expand_dims(genders,axis=1)\n","outputs = np.concatenate([labels, genders], axis=1)\n","\n","print(Xray_images.shape, outputs.shape)"],"metadata":{"id":"OhlrXOCPzpPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(Xray_images, outputs, test_size=0.2, random_state=42)"],"metadata":{"id":"hzy0PsnjzkK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_original = y_train[:,0:2]\n","y_val_original = y_val[:,0:2]"],"metadata":{"id":"u10VxLYp_Ogm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training Samples:\", X_train.shape, y_train_original.shape)\n","print(\"Validation Samples:\", X_val.shape, y_val_original.shape)"],"metadata":{"id":"HIokC_Ow11Tm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16"],"metadata":{"id":"cFFfoQaaTWgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_original = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train_original)))\n","train_dataset_original = train_dataset_original.batch(batch_size)\n","\n","validation_dataset_original = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_val), tf.convert_to_tensor(y_val_original)))\n","validation_dataset_original = validation_dataset_original.batch(batch_size)"],"metadata":{"id":"rjkn4v45NqFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Loop"],"metadata":{"id":"aLkXp_ePGSQD"}},{"cell_type":"code","source":["# Define the CNN model\n","input_shape = (img_size, img_size, 3)\n","\n","original_model = Sequential([\n","    Input(input_shape),\n","    Conv2D(32, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(num_classes, activation='sigmoid')\n","])\n","\n","###################\n","\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","optimizer = Adam()\n","n_epochs = 30\n","\n","val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","\n","for epoch in range(n_epochs):\n","  print(\"\\nEpoch %d\" % (epoch,))\n","  start_time = time.time()\n","  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset_original):\n","    with tf.GradientTape() as tape:\n","        logits = original_model(x_batch_train, training=True)  # Logits for this minibatch\n","        loss_value = loss_fn(y_batch_train[:,0], logits)\n","\n","    grads = tape.gradient(loss_value, original_model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, original_model.trainable_weights))\n","\n","    if step % batch_size == 0:\n","        print(\n","            \"Training loss (for one batch) at step %d: %.4f\"\n","            % (step, float(loss_value))\n","        )\n","        print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n","\n","\n","  for x_batch_val, y_batch_val in validation_dataset_original:\n","      val_logits = original_model(x_batch_val, training=False)\n","      val_acc_metric.update_state(y_batch_val[:,0], val_logits)\n","\n","  val_acc = val_acc_metric.result()\n","  val_acc_metric.reset_state()\n","  print(\"Validation acc: %.4f\" % (float(val_acc),))\n","  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"],"metadata":{"id":"CVhZbiGJPQ5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GradCAM"],"metadata":{"id":"uyyrr_SrSNWH"}},{"cell_type":"code","source":["original_model.summary()"],"metadata":{"id":"khELlZWm1_Q_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layers_names = []\n","for layer in original_model.layers:\n","  print(layer)\n","  if 'conv' in layer.name:\n","    conv_layers_names.append(layer.name)\n","\n","print(\"\\nConvolution layer names:\", conv_layers_names)"],"metadata":{"id":"0x6Dltjo2lxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_conv_layer_name = conv_layers_names[-1]\n","model_last_layer = original_model.layers[-1].name\n","last_conv_layer_name, model_last_layer"],"metadata":{"id":"yHvgtwGQ2MYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batches = []\n","batches_labels = []\n","for x_batch_val, y_batch_val in validation_dataset_original:\n","  img, label = x_batch_val, y_batch_val\n","  batches.append(img)\n","  batches_labels.append(label)"],"metadata":{"id":"9DlB5XaHNLye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(batches_labels), len(batches_labels[0])"],"metadata":{"id":"w7q_XdqONLyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## In this cell, we specify a sample data instance to generate its GradCAM.\n","## Initially, we select a batch from the validation dataset, and then, specify the image index.\n","## The model prediction is done for the whole batch. However, we select the certain\n","## image and its label to produce the GradCAM heatmaps.\n","\n","batch_index = 0\n","img = batches[batch_index]\n","label = batches_labels[batch_index][:,0]\n","print(\"Batch Labels:\", label)\n","print(img.shape, label.shape)\n","\n","index = 0\n","\n","predictions = original_model.predict(img)\n","print(\"Batch Predictions:\", np.argmax(predictions, axis=1))\n","print(\"True Labels:\", np.array(label))\n","\n","print(f\"Batch Index={batch_index}; Image Index:{index}\")\n","\n","# img, label = validation_dataset.get_single_element()\n","img_test = np.expand_dims(img[index], axis=0)\n","print(img_test.shape)\n","pred = original_model.predict(img_test)\n","print(f\"Prediction Vector for Total Batch: {pred}; \\n Predicted Class:{tf.argmax(pred, axis=1)}; True Class:{label[index]}\")"],"metadata":{"id":"Ekg1nO0i8Xmv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM for index 0"],"metadata":{"id":"HG2RI46L0WI_"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, original_model, last_conv_layer_name, model_last_layer, pred_index=0)\n","print(np.mean(heatmap))\n","\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = jet_heatmap * 0.001 + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","# display(superimposed_img)\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"0UV0_7TuV6Jv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM for index 1"],"metadata":{"id":"t-tH1Vhr0nXu"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, original_model, last_conv_layer_name, model_last_layer, pred_index=1)\n","print(np.mean(heatmap))\n","\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = jet_heatmap * 0.001 + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","# display(superimposed_img)\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"Kd75eQKkig6m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM for index 2"],"metadata":{"id":"unBwYZ0L0qmm"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, original_model, last_conv_layer_name, model_last_layer, pred_index=2)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = jet_heatmap * 0.001 + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","# display(superimposed_img)\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"C5mn48BWjzEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fairness"],"metadata":{"id":"I4e5iVq9yHd3"}},{"cell_type":"markdown","source":["Evaluating the fairness of the model using Demographic Parity and Equality Odds metrics.\n","\n","Fairness metrics are typically and principally used for binary classification. Hence, we have to map the true labels into a single label."],"metadata":{"id":"LstnaHcD04Io"}},{"cell_type":"code","source":["## Generating model predictions and recording the true labels for fairness evaluation\n","## Label 0 indicates the true classification label, and label 1 refers to the genders\n","predictions = []\n","true_labels = []\n","\n","for x_batch_val, y_batch_val in validation_dataset_original:\n","  img, label = x_batch_val, y_batch_val\n","  pred = original_model.predict(img)\n","  predictions.append(tf.argmax(pred, axis=1).numpy())\n","  true_labels.append((label[:,0].numpy(), label[:,1].numpy()))"],"metadata":{"id":"rcd1lqVnyMi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = []\n","y_true = []\n","for pred in predictions:\n","  y_pred.extend(pred)\n","\n","for label in true_labels:\n","  for indx in range(len(label[0])):\n","    y_true.append([label[0][indx], label[1][indx]])\n","\n","y_pred = np.array(y_pred)\n","y_true = np.array(y_true)\n","\n","y_pred.shape, y_true.shape"],"metadata":{"id":"pynXrlDVyMjD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy:\", accuracy_score(y_true[:,0], y_pred))"],"metadata":{"id":"CtQsbEpB31IT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\n","## These metrics are for binary classification. Hence, we map the true labels to 1, which indicates the positive case of Pneumonia\n","\n","def binary_map(input_labels):\n","    labels = []\n","    for item in input_labels:\n","      if item != 0:\n","        labels.append(1)\n","      else:\n","        labels.append(item)\n","    return labels\n","\n","## The original multi-class labels\n","# img_preds = y_pred\n","# img_true = y_true[:,0]\n","\n","## The binary classification labels\n","img_preds = binary_map(y_pred)\n","img_true = binary_map(y_true[:,0])\n","gender = [{0:'F', 1:'M'}[l] for l in y_true[:,1]] ## Specifying gender as the protected group\n","\n","print(\"Demographic Parity Difference =\", demographic_parity_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Demographic Parity Ratio =\", demographic_parity_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Multi-class\n","# 0.02083333333333333\n","# 0.75\n","\n","## Binary Mapped Values\n","# 0.09375\n","# 0.625"],"metadata":{"id":"J6vjXZBBzjC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import equalized_odds_difference, equalized_odds_ratio\n","print(\"Equally Odds Difference =\", equalized_odds_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Equally Odds Ratio =\",equalized_odds_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Binary Mapped Values\n","# 0.20833333333333334\n","# 0.4444444444444444"],"metadata":{"id":"ON5VemSEzjDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AUC and SPD"],"metadata":{"id":"yiIcmBsWgRQK"}},{"cell_type":"markdown","source":["Evaluating the model based on AUC and SPD metrics.\n","\n","AUC metric assess the model's accuracy, while SPD examines the model sensitivity towards biases."],"metadata":{"id":"auNejyGe2EfF"}},{"cell_type":"code","source":["from sklearn import metrics\n","from aif360.sklearn.metrics import statistical_parity_difference\n","\n","AUC = metrics.roc_auc_score(img_true, img_preds)\n","print(\"AUC = \", AUC)\n","\n","SPD = statistical_parity_difference(pd.DataFrame(img_true), np.array(img_preds))\n","print(\"SPD = \", SPD)"],"metadata":{"id":"oKJ3dgUigQmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Augmented Learning: Fairness and Explainability"],"metadata":{"id":"pr0fRWVrxlXx"}},{"cell_type":"code","source":["# Xray_images = np.array([img[1] for img in Dataset[key] for key in Dataset.keys()])\n","# labels = np.array([img[4] for img in Dataset[key] for key in Dataset.keys()])\n","# genders = np.array([{'F':0, 'M':1}[img[2]] for img in Dataset[key] for key in Dataset.keys()])\n","# bbox = np.array([img[-1] for img in Dataset[key] for key in Dataset.keys()])\n","\n","Xray_images = []\n","labels = []\n","genders = []\n","bbox = []\n","for key in Dataset.keys():\n","  for img in Dataset[key]:\n","    Xray_images.append(img[1])\n","    genders.append({'F':0, 'M':1}[img[2]])\n","    labels.append(img[4])\n","    bbox.append(img[5])\n","\n","num_classes = len(set(labels))\n","print(\"Num Classes: \", num_classes)\n","\n","Xray_images = np.array(Xray_images)\n","labels = np.array(labels)\n","genders = np.array(genders)\n","bbox = np.array(bbox)\n","\n","bbox_normalized =  bbox / np.linalg.norm(bbox)\n","\n","labels = np.expand_dims(labels,axis=1)\n","genders = np.expand_dims(genders,axis=1)\n","\n","outputs = np.concatenate([labels, genders, bbox_normalized], axis=1)\n","print(Xray_images.shape, labels.shape, genders.shape, bbox.shape, outputs.shape)"],"metadata":{"id":"_DF2dQ0pVi76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs[0:10][:,2:]"],"metadata":{"id":"E3fuZk5IaCo6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into training and validation sets\n","# X_train, X_val, y_train, y_val = train_test_split(Xray_images, outputs, test_size=0.2, shuffle=False)\n","X_train, X_val, y_train, y_val = train_test_split(Xray_images, outputs, test_size=0.2)"],"metadata":{"id":"yhswRfdAVi78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training Samples:\", X_train.shape, y_train.shape)\n","print(\"Validation Samples:\", X_val.shape, y_val.shape)"],"metadata":{"id":"oa8y7wmgVi79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16"],"metadata":{"id":"i62qvPDQUBZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n","train_dataset = train_dataset.batch(batch_size)\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_val), tf.convert_to_tensor(y_val)))\n","validation_dataset = validation_dataset.batch(batch_size)"],"metadata":{"id":"lgNwj3kgUBZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Illustrating the samples in the validation dataset\n","## The code is commented to prevent unnecessary RAM usage.\n","## So, please uncomment the following codes in case the illustrations are required\n","\n","# imgs = []\n","# for x_batch_val, y_batch_val in validation_dataset:\n","#   for img in x_batch_val:\n","#       imgs.append(img)\n","\n","# plt.figure(figsize=(10,10))\n","\n","# i = 0\n","# for img in imgs[0:100]:\n","#   plt.subplot(10, 10, i + 1)\n","#   plt.imshow(img, cmap='gray')\n","#   plt.axis('off')\n","#   i += 1\n","\n","# plt.tight_layout()"],"metadata":{"id":"WTH_JYHcnGtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multiple Outputs AGL Model"],"metadata":{"id":"pm7I3TU8TYkN"}},{"cell_type":"markdown","source":["### Single Optimization"],"metadata":{"id":"vv5pfk7daayd"}},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import *\n","\n","# Difining the CNN model\n","input_shape = (img_size, img_size, 3)\n","inp = Input(shape=input_shape)\n","\n","## Main layers\n","x = Conv2D(32, (3, 3), activation='relu')(inp)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Flatten()(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","\n","## output layers\n","out1 = Dense(num_classes, activation='sigmoid')(x) ## label classification\n","out2 = Dense(2, activation='sigmoid')(x) ## gender-based explanation\n","out3 = Dense(4, activation='sigmoid')(x) # bbox-based explanation\n","\n","model = Model(inp, [out1,out2,out3])\n","\n","## EarlyStopping\n","best_loss = float('inf')\n","best_model_weights = None\n","patience = 10\n","early_stopping = False\n","val_loss_metric = keras.losses.SparseCategoricalCrossentropy()\n","###\n","\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","loss_fn3 = keras.losses.MeanAbsoluteError()\n","optimizer = Adam()\n","\n","n_epochs = 100\n","\n","val_acc_metric1 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric2 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric3 = keras.metrics.MeanAbsoluteError()\n","\n","for epoch in range(n_epochs):\n","  print(\"\\nEpoch %d\" % (epoch,))\n","  start_time = time.time()\n","  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","    with tf.GradientTape() as tape:\n","        logits1, logits2, logits3 = model(x_batch_train, training=True)  # Logits for this minibatch\n","        loss_value1 = loss_fn(y_batch_train[:,0], logits1)\n","        loss_value2 = loss_fn(y_batch_train[:,1], logits2) ## Adding constraint loss for explainability\n","        loss_value3 = loss_fn3(y_batch_train[:,2:], logits3) ## Adding the BBox loss for explainability\n","        loss_value = loss_value1 + loss_value2 + loss_value3\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","    if step % batch_size == 0:\n","        print(\n","            \"Training loss (for one batch) at step %d: %.4f\"\n","            % (step, float(loss_value))\n","        )\n","        print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n","\n","\n","  for x_batch_val, y_batch_val in validation_dataset:\n","      val_logits1, val_logits2, val_logits3 = model(x_batch_val, training=False)\n","      val_acc_metric1.update_state(y_batch_val[:,0], val_logits1)\n","      val_acc_metric2.update_state(y_batch_val[:,1], val_logits2)\n","      val_acc_metric3.update_state(y_batch_val[:,2:], val_logits3)\n","\n","      val_loss = val_loss_metric(y_batch_val[:,0], val_logits1)\n","\n","  val_acc1 = val_acc_metric1.result()\n","  val_acc_metric1.reset_state()\n","\n","  val_acc2 = val_acc_metric2.result()\n","  val_acc_metric2.reset_state()\n","\n","  val_error3 = val_acc_metric3.result()\n","  val_acc_metric3.reset_state()\n","  print(\"Validation acc: %.4f %.4f %.4f\" % (float(val_acc1),float(val_acc2), float(val_error3)))\n","  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n","\n","  if early_stopping:\n","    # Early stopping\n","    if float(val_loss) < best_loss:\n","        best_loss = float(val_loss)\n","        best_model_weights = copy.deepcopy(model.get_weights())  # Deep copy here\n","        patience = 10  # Reset patience counter\n","        print(f\"Early Stopping Restart\")\n","    else:\n","        patience -= 1\n","        print(f\"Early Stopping Patience {patience}\")\n","        if patience == 0:\n","            break\n","\n","model_weights_backup = model.get_weights() ## Storing the model weights as a backup\n","\n","if early_stopping:\n","  model.set_weights(best_model_weights) ## loading the best model from early stopping"],"metadata":{"id":"FLX4DiJ2U16e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Seperate Optimization"],"metadata":{"id":"npFVBOVCaLnB"}},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import *\n","\n","# Defining the CNN model\n","input_shape = (img_size, img_size, 3)\n","inp = Input(shape=input_shape)\n","\n","## Main layers\n","x = Conv2D(32, (3, 3), activation='relu')(inp)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Flatten()(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","\n","## output layers\n","out1 = Dense(num_classes, activation='sigmoid')(x)\n","out2 = Dense(2, activation='sigmoid')(x) ## gender-based explanation\n","out3 = Dense(4, activation='sigmoid')(x) # bbox-based explanation\n","\n","model = Model(inp, [out1,out2,out3])\n","\n","#####################################\n","#####################################\n","\n","loss_fn1 = keras.losses.SparseCategoricalCrossentropy()\n","loss_fn2 = keras.losses.SparseCategoricalCrossentropy()\n","loss_fn3 = keras.losses.MeanAbsoluteError()\n","optimizer1 = Adam()\n","optimizer2 = Adam()\n","optimizer3 = SGD()\n","\n","n_epochs = 100\n","\n","val_acc_metric1 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric2 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric3 = keras.metrics.MeanAbsoluteError()\n","\n","for epoch in range(n_epochs):\n","  print(\"\\nEpoch %d\" % (epoch,))\n","  start_time = time.time()\n","  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","    with tf.GradientTape() as tape0:\n","      with tf.GradientTape(persistent=True) as tape1:\n","          logits1, logits2, logits3 = model(x_batch_train, training=True)  # Logits for this minibatch\n","\n","          loss_value1 = loss_fn1(y_batch_train[:,0], logits1) ## Loss for image classification\n","          loss_value2 = loss_fn2(y_batch_train[:,1], logits2) ## Adding constraint loss for explainability\n","          loss_value3 = loss_fn3(y_batch_train[:,2:], logits3) ## Adding the BBox loss for explainability\n","\n","\n","      variables1 = model.trainable_weights[0:8] ##6,7\n","      variables2 = model.trainable_weights[0:6] + model.trainable_weights[8:10] ##8,9\n","      variables3 = model.trainable_weights[0:6] + model.trainable_weights[10:] ##10,11\n","\n","      grads1 = tape1.gradient(loss_value1, variables1)\n","      optimizer1.apply_gradients(zip(grads1, variables1))\n","\n","      grads2 = tape1.gradient(loss_value2, variables2)\n","      optimizer2.apply_gradients(zip(grads2, variables2))\n","\n","      grads3 = tape1.gradient(loss_value3, variables3)\n","      optimizer3.apply_gradients(zip(grads3, variables3))\n","\n","\n","    if step % batch_size == 0:\n","        print(\n","            \"Training loss (for one batch) at step %d: %.4f %.4f %.4f\"\n","            % (step, float(loss_value1), float(loss_value2), float(loss_value3))\n","        )\n","        print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n","\n","\n","  for x_batch_val, y_batch_val in validation_dataset:\n","      val_logits1, val_logits2, val_logits3 = model(x_batch_val, training=False)\n","      val_acc_metric1.update_state(y_batch_val[:,0], val_logits1)\n","      val_acc_metric2.update_state(y_batch_val[:,1], val_logits2)\n","      val_acc_metric3.update_state(y_batch_val[:,2:], val_logits3)\n","\n","  val_acc1 = val_acc_metric1.result()\n","  val_acc_metric1.reset_state()\n","\n","  val_acc2 = val_acc_metric2.result()\n","  val_acc_metric2.reset_state()\n","\n","  val_error3 = val_acc_metric3.result()\n","  val_acc_metric3.reset_state()\n","  print(\"Validation acc: %.4f %.4f %.4f\" % (float(val_acc1),float(val_acc2), float(val_error3)))\n","  print(\"Time taken: %.2fs\" % (time.time() - start_time))"],"metadata":{"id":"5y6ntQ0-SMpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GradCAMs"],"metadata":{"id":"bxFeaTdQhc_n"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"aIm6qe0o5Tiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layers_names = []\n","for layer in model.layers:\n","  print(layer)\n","  if 'conv' in layer.name:\n","    conv_layers_names.append(layer.name)\n","\n","print(\"\\nConvolution layer names:\", conv_layers_names)"],"metadata":{"id":"14Z92P395Tiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_conv_layer_name = conv_layers_names[-1]\n","model_last_layer = model.layers[-3].name\n","last_conv_layer_name, model_last_layer"],"metadata":{"id":"yijg_65a5Tiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batches = []\n","batches_labels = []\n","for x_batch_val, y_batch_val in validation_dataset:\n","  img, label = x_batch_val, y_batch_val\n","  batches.append(img)\n","  batches_labels.append(label)"],"metadata":{"id":"XZ85Ngl7LxIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(batches_labels), len(batches_labels[0])"],"metadata":{"id":"L7B7j84CL-43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_index = 1\n","img = batches[batch_index]\n","label = batches_labels[batch_index]\n","print(img.shape, label.shape)\n","predictions = model.predict(img)\n","print(\"Batch Predictions:\", np.argmax(predictions[0], axis=1))\n","print(\"True Labels:\", np.array(label[:,0]))\n","\n","index = 4\n","img_test = np.expand_dims(img[index], axis=0)\n","print(img_test.shape)\n","pred1, pred2, pred3 = model.predict(img_test)\n","print(f\"Prediction Vector: {pred1, pred2, pred3}; \\nPredicted Class:{tf.argmax(pred1, axis=1)}; True Class:{label[index][0]}\")"],"metadata":{"id":"uL0pHNOdbnoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index 0"],"metadata":{"id":"XbZtNTsd3_JM"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=0)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"gkGt9_rA5DTj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index1"],"metadata":{"id":"oXBR2rBS4BNT"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=1)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"alHpJRVwN_ke"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index2"],"metadata":{"id":"bhA1Tqdv4C1b"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=2)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"XxNtwhnDN_yO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BBOX"],"metadata":{"id":"Ss9bdYigRp4t"}},{"cell_type":"markdown","source":["Illustrating the true and the predicted BBOXes"],"metadata":{"id":"cRyORnUM4f6s"}},{"cell_type":"code","source":["## The following index is according to the index especified in the GradCAM section.\n","## The image is sampled from the GradCAM selected batch\n","\n","img_sample = img[index]\n","True_Class = label[index][0]\n","## The predicted BBOX requires denormalization due to the original information being normalized to fit the neural network\n","## Therefore, to denormalize the image, we have two option:\n","## 1) use the norm of of original bbox values and multiply it into the predicted values\n","## 2) use the image size and treat the predicted values as the ratios to be mapped on the X-ray images.\n","\n","box_pred = pred3[0]*np.linalg.norm(bbox) #*img_size #\n","box_true = label[index][2:] * np.linalg.norm(bbox)\n","\n","box_true, box_pred"],"metadata":{"id":"K1XsZE32mYpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying a sample image with bbox\n","fig, ax = plt.subplots()\n","ax.imshow(img_sample)\n","print(f\"Annotation: {True_Class}; Predicted: {np.argmax(pred1[0])}\")\n","\n","## Create a Rectangle patch\n","x_true = box_true[0].numpy()\n","y_true = box_true[1].numpy()\n","width_true = box_true[2].numpy()\n","height_true = box_true[3].numpy()\n","\n","x_pred = box_pred[0]\n","y_pred = box_pred[1]\n","width_pred = box_pred[2]\n","height_pred = box_pred[3]\n","\n","print(f\"True Box: {(x_true, y_true, width_true, height_true)}\")\n","\n","rect1 = patches.Rectangle((x_true, y_true), width_true, height_true, linewidth=1, edgecolor='r', facecolor='none')\n","rect2 = patches.Rectangle((x_pred, y_pred), width_pred, height_pred, linewidth=1, edgecolor='b', facecolor='none')\n","\n","ax.add_patch(rect1)\n","ax.add_patch(rect2)\n","plt.tight_layout()"],"metadata":{"id":"yO-toHmXmXys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fairness Metrics, AUC, SPD"],"metadata":{"id":"6plJXJGyrvJn"}},{"cell_type":"code","source":["predictions = []\n","true_labels = []\n","\n","for x_batch_val, y_batch_val in validation_dataset:\n","  img, label = x_batch_val, y_batch_val\n","  pred1, pred2, _ = model.predict(img)\n","  predictions.append((tf.argmax(pred1, axis=1).numpy(), tf.argmax(pred2, axis=1).numpy()))\n","  true_labels.append((label[:,0].numpy(), label[:,1].numpy()))"],"metadata":{"id":"KZw7I5gyryXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = []\n","y_true = []\n","for pred in predictions:\n","  for indx in range(len(pred[0])):\n","    y_pred.append([pred[0][indx], pred[1][indx]])\n","\n","for label in true_labels:\n","  for indx in range(len(label[0])):\n","    y_true.append([label[0][indx], label[1][indx]])\n","\n","y_pred = np.array(y_pred)\n","y_true = np.array(y_true)\n","\n","y_pred.shape, y_true.shape"],"metadata":{"id":"bgbg7iCDs0dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\n","## These metrics are for binary classification\n","\n","def binary_map(input_labels):\n","    labels = []\n","    for item in input_labels:\n","      if item != 0:\n","        labels.append(1)\n","      else:\n","        labels.append(item)\n","    return labels\n","\n","## Multi-class classification\n","# img_preds = y_pred[:,0]\n","# img_true = y_true[:,0]\n","\n","## Binary classification\n","img_preds = binary_map(y_pred[:,0])\n","img_true = binary_map(y_true[:,0])\n","gender = [{0:'F', 1:'M'}[l] for l in y_true[:,1]]\n","\n","print(\"Demographic Parity Difference:\", demographic_parity_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Demographic Parity Ratio:\", demographic_parity_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Multi-class\n","# # 0.03125\n","# # 0.8\n","# 0.02083333333333333\n","# 0.8333333333333334\n","\n","## Binary Mapped Values\n","# 0.03125\n","# 0.875"],"metadata":{"id":"FMtEJROHwIi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import equalized_odds_difference, equalized_odds_ratio\n","print(\"Equality Odds Difference:\", equalized_odds_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Equality Odds Ratio:\", equalized_odds_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Binary Mapped Values\n","# 0.375\n","# 0.5\n","\n","# 0.25\n","# 0.6666666666666666"],"metadata":{"id":"Bvkogy0NsbiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import metrics\n","from aif360.sklearn.metrics import statistical_parity_difference\n","\n","AUC = metrics.roc_auc_score(img_true, img_preds)\n","print(\"AUC = \", AUC)\n","\n","SPD = statistical_parity_difference(pd.DataFrame(img_true), np.array(img_preds))\n","print(\"SPD = \", SPD)"],"metadata":{"id":"_u6XguxbjNo8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Enhanced Augmented Learning: Fairness and Explainability"],"metadata":{"id":"oDtbuHdVmRps"}},{"cell_type":"markdown","source":["### Data Preparation"],"metadata":{"id":"Sdeqxkv0nlZe"}},{"cell_type":"code","source":["Xray_images = []\n","labels = []\n","genders = []\n","bbox = []\n","for key in Dataset.keys():\n","  for img in Dataset[key]:\n","    Xray_images.append(img[1])\n","    genders.append({'F':0, 'M':1}[img[2]])\n","    labels.append(img[4])\n","    bbox.append(img[5])\n","\n","num_classes = len(set(labels))\n","print(\"Num Classes: \", num_classes)\n","\n","Xray_images = np.array(Xray_images)\n","labels = np.array(labels)\n","genders = np.array(genders)\n","bbox = np.array(bbox)\n","\n","# bbox_normalized1 = normalize(bbox, ord=1)\n","bbox_normalized =  bbox / np.linalg.norm(bbox)\n","# bbox_denormalized = bbox_normalized2 * np.linalg.norm(bbox)\n","\n","labels = np.expand_dims(labels,axis=1)\n","genders = np.expand_dims(genders,axis=1)\n","\n","outputs = np.concatenate([labels, genders, bbox_normalized], axis=1)\n","print(Xray_images.shape, labels.shape, genders.shape, bbox.shape,outputs.shape)"],"metadata":{"id":"kV_5M_dgJ3NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into training and validation sets\n","# X_train, X_val, y_train, y_val = train_test_split(Xray_images, outputs, test_size=0.2, shuffle=False)\n","X_train, X_val, y_train, y_val = train_test_split(Xray_images, outputs, test_size=0.2, random_state=42)"],"metadata":{"id":"8HwEpK_unlZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training Samples:\", X_train.shape, y_train.shape)\n","print(\"Validation Samples:\", X_val.shape, y_val.shape)"],"metadata":{"id":"EncclN-rnlZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16"],"metadata":{"id":"VGdBADVZnlZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n","train_dataset = train_dataset.batch(batch_size)\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_val), tf.convert_to_tensor(y_val)))\n","validation_dataset = validation_dataset.batch(batch_size)"],"metadata":{"id":"9SYhCvlunlZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Model"],"metadata":{"id":"cV4bstrIOwiA"}},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import *\n","\n","\n","cons_landas = {key: 0 for key in range(2)}\n","meu = 0.5\n","C = 0.8\n","delta = 0.001\n","threshold = 0.5\n","\n","# Define the CNN model\n","input_shape = (img_size, img_size, 3)\n","inp = Input(shape=input_shape)\n","\n","## Main layers\n","x = Conv2D(32, (3, 3), activation='relu')(inp)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), activation='relu')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Flatten()(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","\n","## output layers\n","out1 = Dense(num_classes, activation='sigmoid')(x)\n","out2 = Dense(2, activation='sigmoid')(x) ## gender-based explanation\n","out3 = Dense(4, activation='sigmoid')(x) # bbox-based explanation\n","\n","model = Model(inp, [out1,out2,out3])\n","\n","## EarlyStopping\n","best_loss = float('inf')\n","const_best_model_weights = None\n","patience_reset = 10\n","patience = patience_reset\n","early_stopping = False\n","val_loss_metric = keras.losses.SparseCategoricalCrossentropy()\n","\n","###\n","\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","loss_fn3 = keras.losses.MeanAbsoluteError()\n","optimizer = Adam()\n","\n","n_epochs = 30\n","\n","val_acc_metric1 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric2 = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric3 = keras.metrics.MeanAbsoluteError()\n","\n","for epoch in range(n_epochs):\n","  print(\"\\nEpoch %d\" % (epoch,))\n","  start_time = time.time()\n","  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","    with tf.GradientTape() as tape:\n","        logits1, logits2, logits3 = model(x_batch_train, training=True)  # Logits for this minibatch\n","        loss_value1 = loss_fn(y_batch_train[:,0], logits1)\n","\n","        loss_value2 = loss_fn(y_batch_train[:,1], logits2) ## Adding Gender constraint loss for explainability\n","        loss_value3 = loss_fn3(y_batch_train[:,2:6], logits3) ## Adding the BBox loss for explainability\n","\n","        consts_loss = [loss_value2, loss_value3]\n","        consts_0 = cons_landas[0]*consts_loss[0] + (meu/2)*(max(0, consts_loss[0])**2)\n","        consts_1 = cons_landas[1]*consts_loss[1] + (meu/2)*(max(0, consts_loss[1])**2)\n","        final_loss = sum([consts_0, consts_1])\n","\n","        theta_loss = 0\n","        for weight in model.get_weights():\n","          theta_loss += delta*tf.nn.l2_loss(weight)\n","\n","        loss_value = loss_value1 + theta_loss + final_loss\n","\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","\n","    cons_landas[0] += meu*max(0, consts_0)\n","    cons_landas[1] += meu*max(0, consts_1)\n","\n","    sum_consts = 0\n","    sum_consts += max(0, consts_0)**2\n","    sum_consts += max(0, consts_1)**2\n","\n","    if sum_consts > threshold:\n","      meu *= C\n","\n","    if step % batch_size == 0:\n","        print(\n","            \"Training loss (for one batch) at step %d: %.4f %.4f %.4f %.4f\"\n","            % (step, float(loss_value1), float(loss_value), float(final_loss), float(theta_loss))\n","        )\n","        print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n","\n","\n","  for x_batch_val, y_batch_val in validation_dataset:\n","      val_logits1, val_logits2, val_logits3 = model(x_batch_val, training=False)\n","      val_acc_metric1.update_state(y_batch_val[:,0], val_logits1)\n","      val_acc_metric2.update_state(y_batch_val[:,1], val_logits2)\n","      val_acc_metric3.update_state(y_batch_val[:,2:6], val_logits3)\n","\n","      val_loss = val_loss_metric(y_batch_val[:,0], val_logits1)\n","\n","  val_acc1 = val_acc_metric1.result()\n","  val_acc_metric1.reset_state()\n","\n","  val_acc2 = val_acc_metric2.result()\n","  val_acc_metric2.reset_state()\n","\n","  val_error3 = val_acc_metric3.result()\n","  val_acc_metric3.reset_state()\n","  print(\"Validation acc: %.4f %.4f %.4f %.4f\" % (float(val_acc1),float(val_acc2), float(val_error3), float(val_loss)))\n","  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n","\n","  if early_stopping:\n","    # Early stopping\n","    if float(val_loss) < best_loss:\n","        best_loss = float(val_loss)\n","        const_best_model_weights = copy.deepcopy(model.get_weights())  # Deep copy here\n","        patience = patience_reset  # Reset patience counter\n","        print(f\"Early Stopping Restart\")\n","    else:\n","        patience -= 1\n","        print(f\"Early Stopping Patience {patience}\")\n","        if patience == 0:\n","            break\n","\n","\n","model_backup = copy.deepcopy(model)\n","\n","if early_stopping:\n","  model.set_weights(const_best_model_weights)\n","  print(\"Best Loss:\", best_loss)"],"metadata":{"id":"O4WCJh2F5w-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.set_weights(model_backup.get_weights())"],"metadata":{"id":"F7GPcttkOjIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GradCAMs"],"metadata":{"id":"UVhSXWHynlZo"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"0nIkavHsnlZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layers_names = []\n","for layer in model.layers:\n","  print(layer)\n","  if 'conv' in layer.name:\n","    conv_layers_names.append(layer.name)\n","\n","print(\"\\nConvolution layer names:\", conv_layers_names)"],"metadata":{"id":"J8k-81DYnlZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_conv_layer_name = conv_layers_names[-1]\n","model_last_layer = model.layers[-3].name\n","last_conv_layer_name, model_last_layer"],"metadata":{"id":"X5_nxPQonlZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batches = []\n","batches_labels = []\n","for x_batch_val, y_batch_val in validation_dataset:\n","  img, label = x_batch_val, y_batch_val\n","  batches.append(img)\n","  batches_labels.append(label)"],"metadata":{"id":"rC3-XxIsnlZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(batches_labels), len(batches_labels[0])"],"metadata":{"id":"uWIyVNvJnlZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_index = 0\n","img = batches[batch_index]\n","label = batches_labels[batch_index]\n","\n","predictions = model.predict(img)\n","print(\"Batch Predictions:\", np.argmax(predictions[0], axis=1))\n","print(\"True Labels:\", np.array(label[:,0]))\n","\n","index = 0\n","print(f\"Batch Index: {batch_index}; Image Index: {index}\")\n","# img, label = validation_dataset.get_single_element()\n","img_test = np.expand_dims(img[index], axis=0)\n","pred1, pred2, pred3 = model.predict(img_test)\n","print(f\"Prediction Vector: {pred1, pred2, pred3}; \\nPredicted Class:{tf.argmax(pred1, axis=1)}; True Class:{label[index][0]}\")"],"metadata":{"id":"EsrFBE9hnlZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index 0"],"metadata":{"id":"F7DxNKAZ6kDg"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=0)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"BbeQuKgxnlZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index 1"],"metadata":{"id":"0Pbb7CM66l0g"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=1)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"_y963T4vnlZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GradCAM index 2"],"metadata":{"id":"SNIqmNE16nfZ"}},{"cell_type":"code","source":["heatmap = gradcam(img_test, model, last_conv_layer_name, model_last_layer, pred_index=2)\n","print(np.mean(heatmap))\n","#######################\n","heatmap_ = np.uint8(255 * heatmap)\n","jet = mpl.colormaps[\"jet\"]\n","\n","# Use RGB values of the colormap\n","jet_colors = jet(np.arange(256))[:, :3]\n","jet_heatmap = jet_colors[heatmap_]\n","\n","# Create an image with RGB colorized heatmap\n","jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","jet_heatmap = jet_heatmap.resize((img_test.shape[1], img_test.shape[1]))\n","jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","print(img_test[0].shape, jet_heatmap.shape)\n","\n","# Superimpose the heatmap on original image\n","superimposed_img = 0.002*jet_heatmap + img_test[0]\n","superimposed_img = keras.utils.array_to_img(superimposed_img)\n","superimposed_img = superimposed_img.resize((256, 256))\n","\n","# Display Grad CAM\n","fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].matshow(heatmap_)\n","axes[0].set_title(\"Heatmap\")\n","axes[0].axis('off')\n","axes[1].imshow(superimposed_img)\n","axes[1].set_title(\"Super Imposed Image\")\n","axes[1].axis('off')\n","plt.show()"],"metadata":{"id":"HWsTGfJVnlZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BBOX"],"metadata":{"id":"pHV3Zel-nlZs"}},{"cell_type":"markdown","source":["Illustrating a sample BBOX on an X-ray image"],"metadata":{"id":"HOmsOc_t6qdZ"}},{"cell_type":"code","source":["img_sample = img[index]\n","True_Class = label[index][0]\n","## The predicted BBOX requires denormalization due to the original information being normalized to fit the neural network\n","## Therefore, to denormalize the image, we have two option:\n","## 1) use the norm of of original bbox values and multiply it into the predicted values\n","## 2) use the image size and treat the predicted values as the ratios to be mapped on the X-ray images.\n","\n","\n","box_pred = pred3[0]*np.linalg.norm(bbox)\n","# box_pred = pred3[0]*img_size\n","\n","d = pred3[0]*np.linalg.norm(bbox)\n","print(\"Denormalized Box info:\", d)\n","box_true = label[index][2:] * np.linalg.norm(bbox)\n","\n","box_true, box_pred"],"metadata":{"id":"fO7rF-UCnlZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Displaying a sample image with bbox\n","fig, ax = plt.subplots()\n","ax.imshow(img_sample)\n","print(f\"Annotation: {True_Class}; Predicted: {np.argmax(pred1[0])}\")\n","\n","## Create a Rectangle patch\n","x_true = box_true[0].numpy()\n","y_true = box_true[1].numpy()\n","width_true = box_true[2].numpy()\n","height_true = box_true[3].numpy()\n","\n","x_pred = box_pred[0]\n","y_pred = box_pred[1]\n","width_pred = box_pred[2]\n","height_pred = box_pred[3]\n","\n","print(f\"True Box: {(x_true, y_true, width_true, height_true)}\")\n","\n","rect1 = patches.Rectangle((x_true, y_true), width_true, height_true, linewidth=1, edgecolor='r', facecolor='none')\n","rect2 = patches.Rectangle((x_pred, y_pred), width_pred, height_pred, linewidth=1, edgecolor='b', facecolor='none')\n","\n","ax.add_patch(rect1)\n","ax.add_patch(rect2)\n","plt.tight_layout()"],"metadata":{"id":"krE13Zz1nlZt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fairness Metrics"],"metadata":{"id":"jIFMhMSLnlZt"}},{"cell_type":"code","source":["predictions = []\n","true_labels = []\n","\n","## If the next code threw an error unexpectedly, this code will work.\n","for x_batch_val, y_batch_val in validation_dataset:\n","  img, label = x_batch_val, y_batch_val\n","  pred1, pred2, _ = model.predict(img)\n","  predictions.append((tf.argmax(pred1, axis=1).numpy(), tf.argmax(pred2, axis=1).numpy()))\n","  true_labels.append((label[:,0].numpy(), label[:,1].numpy()))"],"metadata":{"id":"BHbM69__nlZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = []\n","y_true = []\n","for pred in predictions:\n","  for indx in range(len(pred[0])):\n","    y_pred.append([pred[0][indx], pred[1][indx]])\n","\n","for label in true_labels:\n","  for indx in range(len(label[0])):\n","    y_true.append([label[0][indx], label[1][indx]])\n","\n","y_pred = np.array(y_pred)\n","y_true = np.array(y_true)\n","\n","y_pred.shape, y_true.shape"],"metadata":{"id":"EsVhpUjznlZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy: \", accuracy_score(y_true[:,0], y_pred[:,0]))"],"metadata":{"id":"Dmg6Kmho3k6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\n","## These metrics are for binary classification\n","\n","def binary_map(input_labels):\n","    labels = []\n","    for item in input_labels:\n","      if item != 0:\n","        labels.append(1)\n","      else:\n","        labels.append(int(item))\n","    return labels\n","\n","## Multi-class predictions\n","# img_preds = y_pred[:,0]\n","# img_true = y_true[:,0]\n","\n","## Binary classification\n","img_preds = binary_map(y_pred[:,0])\n","img_true = binary_map(y_true[:,0])\n","gender = [{0:'F', 1:'M'}[l] for l in y_true[:,1]]\n","\n","print(\"Demographic Parity Difference =\", demographic_parity_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Demographic Parity Ratio =\",demographic_parity_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Multi-class\n","## 0.03125\n","## 0.8\n","# 0.02083333333333333\n","# 0.8333333333333334\n","\n","## Binary Mapped Values\n","# 0.03125\n","# 0.875"],"metadata":{"id":"yNtViySunlZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairlearn.metrics import equalized_odds_difference, equalized_odds_ratio\n","print(\"Equally Odds Difference =\", equalized_odds_difference(img_true, img_preds, sensitive_features=gender))\n","print(\"Equally Odds Ratio =\", equalized_odds_ratio(img_true, img_preds, sensitive_features=gender))\n","\n","## Binary Mapped Values\n","# 0.375\n","# 0.5\n","\n","# 0.25\n","# 0.6666666666666666"],"metadata":{"id":"quKmf0YGnlZv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AUC and SPD"],"metadata":{"id":"n-GsZ0tJuqDS"}},{"cell_type":"code","source":["from sklearn import metrics\n","from aif360.sklearn.metrics import statistical_parity_difference\n","\n","AUC = metrics.roc_auc_score(img_true, img_preds)\n","print(\"AUC = \", AUC)\n","\n","SPD = statistical_parity_difference(pd.DataFrame(img_true), np.array(img_preds))\n","print(\"SPD = \", SPD)"],"metadata":{"id":"zinRRnsKuqDU"},"execution_count":null,"outputs":[]}]}